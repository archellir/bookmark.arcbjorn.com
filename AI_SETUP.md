# AI Model Setup Guide

## ğŸ¤– 3-Layer AI Architecture

The AI system uses a 3-layer approach:
1. **Layer 1**: Rule-based (always active)
2. **Layer 2**: FastText lightweight ML
3. **Layer 3**: ONNX advanced content understanding

## ğŸ“ Model Directory Structure

```
./models/
â”œâ”€â”€ fasttext/
â”‚   â”œâ”€â”€ model.bin          # FastText model file
â”‚   â”œâ”€â”€ labels.txt         # Category labels
â”‚   â””â”€â”€ vocab.txt          # Vocabulary file
â””â”€â”€ onnx/
    â”œâ”€â”€ sentiment.onnx     # Sentiment analysis model
    â”œâ”€â”€ topic.onnx         # Topic classification model
    â””â”€â”€ quality.onnx       # Content quality model
```

## ğŸš€ Quick Start

### 1. Create Model Directories
```bash
mkdir -p ./models/fasttext
mkdir -p ./models/onnx
```

### 2. Environment Variables (Optional)
```bash
export FASTTEXT_MODEL_PATH="./models/fasttext"
export ONNX_MODEL_PATH="./models/onnx"
export AI_LAYER2_ENABLED="true"
export AI_LAYER3_ENABLED="true"
```

### 3. Start Application
```bash
./torimemo
```

The AI system will:
- âœ… Always use Layer 1 (rule-based)
- âš ï¸ Use Layer 2 if FastText models found
- âš ï¸ Use Layer 3 if ONNX models found
- ğŸ”„ Fallback to rule-based if models missing

## ğŸ“¥ Model Sources

### FastText Models
Download pre-trained models:
```bash
# Option 1: Official FastText models
wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin -O ./models/fasttext/model.bin

# Option 2: Train custom model
# See FastText documentation for training
```

### ONNX Models
```bash
# Hugging Face models (convert to ONNX)
pip install transformers onnx

# Export sentiment model
python -m transformers.onnx --model=cardiffnlp/twitter-roberta-base-sentiment-latest ./models/onnx/sentiment/

# Export topic model  
python -m transformers.onnx --model=facebook/bart-large-mnli ./models/onnx/topic/
```

## ğŸ”§ API Endpoints

Test AI functionality:

```bash
# Check AI status
curl http://localhost:8080/api/ai/status

# Get tag suggestions
curl -X POST http://localhost:8080/api/ai/categorize \
  -H "Content-Type: application/json" \
  -d '{"url":"https://example.com","title":"Example","description":"Test"}'

# Find duplicates
curl http://localhost:8080/api/ai/duplicates/123

# Get clusters
curl http://localhost:8080/api/ai/clusters

# Predictive suggestions
curl http://localhost:8080/api/ai/predict/tags?user_id=1
```

## ğŸ” Debugging

### Check Model Status
```bash
curl http://localhost:8080/api/ai/status | jq
```

### View Logs
```bash
LOG_LEVEL=DEBUG ./torimemo
```

### Test Individual Layers
- Layer 1: Always works (rule-based)
- Layer 2: Check `./models/fasttext/model.bin` exists
- Layer 3: Check `./models/onnx/*.onnx` exist

## ğŸ“Š Performance

| Layer | Latency | Accuracy | Resource Usage |
|-------|---------|----------|----------------|
| 1 (Rules) | <1ms | 70% | Minimal |
| 2 (FastText) | ~10ms | 85% | Low |
| 3 (ONNX) | ~100ms | 95% | Moderate |

## ğŸ¯ Production Tips

1. **Start with Layer 1**: Always works, good baseline
2. **Add Layer 2**: Significant accuracy boost, low overhead
3. **Add Layer 3**: Best accuracy, higher latency
4. **Monitor**: Use `/api/ai/status` for health checks
5. **Fallbacks**: System gracefully degrades if models fail

## ğŸ› ï¸ Custom Models

### Training FastText
```bash
# Prepare training data
echo "__label__tech This is about technology" > train.txt
echo "__label__news This is news content" >> train.txt

# Train model
fasttext supervised -input train.txt -output ./models/fasttext/model
```

### Converting to ONNX
```python
import torch
from transformers import AutoModel, AutoTokenizer
import onnx

model = AutoModel.from_pretrained("your-model")
torch.onnx.export(model, inputs, "model.onnx")
```

## ğŸ“ Configuration

Default settings in `internal/ai/categorizer.go`:
- FastText confidence threshold: 0.7
- ONNX confidence threshold: 0.8
- Similarity threshold: 0.85
- Max suggestions: 5

Override via environment or config file.